{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f4fcfe4",
      "metadata": {
        "id": "8f4fcfe4"
      },
      "source": [
        "# Travail personnel : Algorithmique et Python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BONZI Doria - M1 Industries de la langue - 11/11/2023\n",
        "\n",
        "Ce notebook sert à traiter des fichiers afin d'afficher leurs nombres de mots et de phrases et regroupent ces statistiques dans un fichier tsv."
      ],
      "metadata": {
        "id": "e2hT6B3I7IrF"
      },
      "id": "e2hT6B3I7IrF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importation des modules et téléchargement de fichiers test"
      ],
      "metadata": {
        "id": "6w52Hcnl0ScI"
      },
      "id": "6w52Hcnl0ScI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f1336f",
      "metadata": {
        "id": "53f1336f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1Y6h0iynK2ZsDe5Ig5t93w-I6YbtxOidG -O LOREM.txt\n",
        "!gdown --id 1JTkhHmeuS3M7zBJhTSPEFVgTEUk6escq -O MARIO.txt\n",
        "!gdown --id 1tg0SfXFlIwrynyMlTh5g2vgOacsQNAFe -O DOVERBEACH.txt"
      ],
      "metadata": {
        "id": "F3xKKNJfrb9j"
      },
      "id": "F3xKKNJfrb9j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def segment_text(input_file, output_file_sentences, output_file_words):\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    #Segmentation en phrases\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    with open(output_file_sentences, 'w', encoding='utf-8') as output_sentences:\n",
        "        for sentence in sentences:\n",
        "            output_sentences.write(sentence + \" $ \" + '\\n')\n",
        "\n",
        "    #Tokénisation\n",
        "    with open(output_file_sentences, 'r', encoding='utf-8') as segmented_file:\n",
        "        segmented_text = segmented_file.read()\n",
        "        words = nltk.word_tokenize(segmented_text)\n",
        "        with open(output_file_words, 'w', encoding='utf-8') as output_words:\n",
        "            for word in words:\n",
        "                output_words.write(word + '\\n')\n",
        "\n",
        "segment_text('MARIO.txt', 'phrases.txt', 'mots.txt')"
      ],
      "metadata": {
        "id": "EUej3uPLJJLD"
      },
      "id": "EUej3uPLJJLD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ea191e2e",
      "metadata": {
        "id": "ea191e2e"
      },
      "source": [
        "### Segmentation en phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc65059f",
      "metadata": {
        "id": "dc65059f"
      },
      "source": [
        "Une fonction de segmentation en phrases (avec suppression des espaces ou retour chariot entre phrases)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\"\"\" fonction de segmentation en phrases\n",
        "arg1 : text: le texte à segmenter\n",
        "retour : une liste de phrases (string)\n",
        "\"\"\"\n",
        "segment(text:str)->list[str]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qKlyQeec0uCZ"
      },
      "id": "qKlyQeec0uCZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc5b711",
      "metadata": {
        "id": "abc5b711"
      },
      "outputs": [],
      "source": [
        "exemple=\"\"\"Plusieurs mois plus tard, Mia croise à nouveau Sebastian à une pool party où celui-ci, peu investi, reprend avec un groupe des succès des années 1980. Elle se moque de lui et de sa carrière stagnante ; il réplique en ironisant sur ses aspirations d'actrice. Lorsqu'ils repartent ensemble chercher leurs voitures, ils se plaignent d'avoir eu à se recroiser, mais partagent bientôt une danse devant une vue de la ville (A Lovely Night). Sebastian parvient à lui rendre visite à son café et devenus confidents, ils se font découvrir leurs centres d'intérêt respectifs : Mia lui fait part de la passion ancienne qu'elle éprouve pour les studios qu'ils visitent et plus généralement pour le cinéma ; il l'entraîne dans un club de jazz où il lui avoue son rêve d'ouvrir un jour son propre établissement. Ces moments scellent leur profonde amitié (City of Stars). Sebastian invite alors Mia à une projection de La Fureur de vivre. Elle accepte, dans un premier temps, avant de se voir rappeler au dernier moment qu'elle a déjà un rendez-vous avec Greg, son petit ami. Mais le dîner avec Greg et ses proches se révèle ennuyeux. Elle s'enfuit et court rejoindre Sebastian au cinéma, alors que la séance vient de commencer. Leur soirée s'achève par une nouvelle danse dans le planétarium de l'observatoire Griffith, aperçu dans le film qu'ils viennent de quitter (\"Planetarium\"). C'est là qu'ils s'embrassent pour la première fois.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a275935",
      "metadata": {
        "scrolled": true,
        "id": "9a275935"
      },
      "outputs": [],
      "source": [
        "def segment(text:str)->list[str]:\n",
        "    sentences=[]\n",
        "    current_sent=\"\"\n",
        "    for i,char in enumerate(text):\n",
        "        #si on trouve une frontière de phrase\n",
        "        if char==\" \" and (text[i-1] in \"?!.\" and text[i+1].isupper()):\n",
        "\n",
        "            sentences.append(current_sent)\n",
        "            current_sent=\"\"\n",
        "\n",
        "        else:\n",
        "            current_sent+=char\n",
        "\n",
        "    if current_sent!=\"\":\n",
        "        sentences.append(current_sent)\n",
        "\n",
        "    print(\"Nb de phrases:\",len(sentences))\n",
        "    return sentences\n",
        "\n",
        "#texte_segmente=segment(exemple)\n",
        "#print(texte_segmente)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31717da0",
      "metadata": {
        "id": "31717da0"
      },
      "source": [
        "Pour la segmentation en phrases, écrire en pseudo-code une deuxième version permettant de gérer les abréviations, à partir d'une liste définissant une liste d'abréviations à traiter :"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Pour les abréviations, nous avons remarqué par exemple avec notre fichier LOREM.txt le traitement de deux phrases différentes avec l'abréviation \"[...] de 45 av. J.-C, [...]\". Nous écrivons un pseudo-code qui permettrait de gérer ce type d'abréviations.\n",
        "\n",
        "string_abrevs ← [\"M\",\"cf\",\"etc\",\"ex\",\"av\", \"J\"...]\n",
        "\n",
        "Fonction segmentation(texte, abrevs):\n",
        "    phrases ← []\n",
        "    phrase_en_cours ← \"\"\n",
        "\n",
        "    Pour chaque caractère c dans le texte {\n",
        "        Si caractère c est un espace \" \" ET est [précédé par un séparateur de phrase (point, point d'interrogation, point d'exclamation) et suivi d'une majuscule] {\n",
        "            Ajouter caractère c à phrase_en_cours\n",
        "\n",
        "            Si phrase_en_cours n'est pas vide et le mot suivant est une abréviation {\n",
        "                Continuer la construction de la phrase\n",
        "            } Sinon {\n",
        "                Ajouter phrase_en_cours à la liste des phrases\n",
        "                Réinitialiser phrase_en_cours à une chaîne vide\n",
        "            } Sinon {\n",
        "                Ajouter caractère c à phrase_en_cours\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    Retourner la liste des phrases\n",
        "```"
      ],
      "metadata": {
        "id": "eKxDMYed6VC1"
      },
      "id": "eKxDMYed6VC1"
    },
    {
      "cell_type": "markdown",
      "id": "775a59d6",
      "metadata": {
        "id": "775a59d6"
      },
      "source": [
        "### Tokenisation avec suppression des espaces"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e4462c1",
      "metadata": {
        "id": "4e4462c1"
      },
      "source": [
        "Une fonction de tokenisation d'une chaînes en tokens (avec suppression des espaces) :"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\"\"\" fonction de tokenisation\n",
        "arg1 : text: le texte à tokeniser\n",
        "retour : une liste de tokens (str)\n",
        "\"\"\"\n",
        "tokenize(text:str)->list[str]\n",
        "```\n"
      ],
      "metadata": {
        "id": "2bc8mpMP6ZHl"
      },
      "id": "2bc8mpMP6ZHl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "801dfb9e",
      "metadata": {
        "id": "801dfb9e"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentences:list)->list:\n",
        "    prev_char=\" \"\n",
        "    current_word=\"\"\n",
        "    tokens=[]\n",
        "    nb_words=0\n",
        "\n",
        "    for char in sentences:\n",
        "        if  (char in string.punctuation+\" \"):\n",
        "            if not (prev_char in string.punctuation+\" \"):\n",
        "                nb_words=nb_words+1\n",
        "\n",
        "                if char==\"'\":\n",
        "                    current_word+=char\n",
        "                tokens.append(current_word)\n",
        "                current_word=\"\"\n",
        "\n",
        "            if char!=\" \" and char!=\"'\":\n",
        "                tokens.append(char)\n",
        "        else:\n",
        "            current_word=current_word+char\n",
        "\n",
        "        prev_char=char\n",
        "\n",
        "    if current_word!=\"\":\n",
        "        tokens.append(current_word)\n",
        "\n",
        "    print(\"Nb de mots:\",nb_words)\n",
        "    return tokens\n",
        "\n",
        "#texte_tokenise=tokenize(exemple)\n",
        "#print(texte_tokenise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf5f9603",
      "metadata": {
        "id": "cf5f9603"
      },
      "outputs": [],
      "source": [
        "#Un test avec le texte d'exemple défini au début de ce notebook\n",
        "\n",
        "#segmentation=segment(exemple)\n",
        "#tokenisation=tokenize(exemple)\n",
        "#print(segmentation)\n",
        "#print(tokenisation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa00ef40",
      "metadata": {
        "id": "aa00ef40"
      },
      "source": [
        "### Lecture d'un répertoire"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e74afc6",
      "metadata": {
        "id": "5e74afc6"
      },
      "source": [
        "Une fonction de lecture d'un répertoire qui, pour chaque fichier NAME.txt crée un fichier NAME.seg.json contenant une liste de paragraphes, chaque paragraphes étant une liste de phrases, chaque phrases étant une liste de tokens. En outre, le script enregistrera dans un fichier stats.tsv les statististiques de chaque fichiers, selon les colonnes suivantes :"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\"\"\" fonction de traitement d'un répertoire\n",
        "arg1 : dir_name : le chemin vers le répertoire\n",
        "retour : n le nombre fichiers traités + écriture dans le fichiers stats.tsv + écriture des fichiers NAME.seg.json\n",
        "\"\"\"\n",
        "process_dir(dir_name:str,stats_file_name:str)→int :\n",
        "```"
      ],
      "metadata": {
        "id": "nPVbTrsW6lBF"
      },
      "id": "nPVbTrsW6lBF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Contenu de stats.tsv :\n",
        "\n",
        "File\t        nbParas\t nbSents\tnbToks\tnbChars\n",
        "exemple1.txt\t10\t     25\t        221\t    842\n",
        "exemple2.txt\t5\t     10\t        98\t    419\n",
        "```"
      ],
      "metadata": {
        "id": "zRCIZgd26nqs"
      },
      "id": "zRCIZgd26nqs"
    },
    {
      "cell_type": "markdown",
      "id": "20aad37f",
      "metadata": {
        "id": "20aad37f"
      },
      "source": [
        "#### [test] Sur un fichier donné"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "#On ouvre et on lit le fichier texte \"LOREM\", on stocke chaque ligne dans une liste \"fichier\".\n",
        "fichier=[]\n",
        "\n",
        "try:\n",
        "    f=open('LOREM.txt', 'r')\n",
        "\n",
        "#lecture des lignes (paragraphes)\n",
        "    for ligne in f:\n",
        "        fichier.append(ligne.strip())\n",
        "    print(fichier)\n",
        "\n",
        "    f.close()\n",
        "\n",
        "except OSError as err:\n",
        "    print(\"OS error:\",err)\n",
        "except ValueError:\n",
        "    print(\"could not convert data to an integer\")\n",
        "except Exception as err:\n",
        "    print(\"Unexcepted {err=}, {type(err)=}\")\n",
        "    raise\n",
        "```"
      ],
      "metadata": {
        "id": "EV_FOvoA6qZl"
      },
      "id": "EV_FOvoA6qZl"
    },
    {
      "cell_type": "markdown",
      "id": "27bcb407",
      "metadata": {
        "id": "27bcb407"
      },
      "source": [
        "#### [test] Utilisation des fonctions définies précédemment sur un fichier donné"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "#La liste \"fichier\" n'est pas exploitable en tant que liste : on joint les éléments de la liste pour former un texte exploitable.\n",
        "texte=\" \".join(fichier)\n",
        "\n",
        "def process_text(texte: str)->list:\n",
        "    #Segmentation du texte contenu dans le fichier\n",
        "    sentences=segment(texte)\n",
        "\n",
        "    #Tokénisation de la sortie \"Segmentation\"\n",
        "    tokens_list=[]\n",
        "    for sentence in sentences:\n",
        "        tokens=tokenize(sentence)\n",
        "        tokens_list.append(tokens)\n",
        "    return tokens_list\n",
        "\n",
        "texte_traite=process_text(texte)\n",
        "print(texte_traite)\n",
        "\n",
        "#Création du fichier avec le texte segmenté et tokénisé\n",
        "with open('texte_traite.seg.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(texte_traite, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "#Création du fichier stats\n",
        "with open('texte_traite_stats.tsv', 'w', encoding='utf-8') as stats_file:\n",
        "    stats_file.write(\"Fichier\\tParagraphs\\tSentences\\tTokens\\n\")\n",
        "    nb_paragraphes=len(fichier)\n",
        "    nb_phrases=len(texte_traite)\n",
        "    nb_tokens=sum(len(sentence) for sentence in texte_traite)\n",
        "    stats_file.write(f\"LOREM.txt\\t{nb_paragraphes}\\t{nb_phrases}\\t{nb_tokens}\\n\")\n",
        "```"
      ],
      "metadata": {
        "id": "qcdicONO6uKF"
      },
      "id": "qcdicONO6uKF"
    },
    {
      "cell_type": "markdown",
      "id": "e72e0e70",
      "metadata": {
        "id": "e72e0e70"
      },
      "source": [
        "### Création d'une fonction pour traiter les fichiers txt d'un répertoire avec création d'un fichier seg.json et stats.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540d6a62",
      "metadata": {
        "id": "540d6a62"
      },
      "outputs": [],
      "source": [
        "def process_dir(dir_name:str, stats_file_name:str)->int:\n",
        "\n",
        "    #On ouvre le fichier de statistiques \"stats.tsv\"\n",
        "    with open(stats_file_name, 'w', encoding='utf-8') as stats_file:\n",
        "        stats_file.write(\"Filename\\tParagraphes\\tPhrases\\tTokens\\tCaractères\\n\")\n",
        "\n",
        "        fichiers_traites=0\n",
        "\n",
        "        #On parcourt tous les fichiers du répertoire, et si le fichier termine par \".txt\", on le traite comme \"input_file\"\n",
        "        for filename in os.listdir(dir_name):\n",
        "            if filename.endswith('.txt'):\n",
        "                input_file=os.path.join(dir_name, filename)\n",
        "\n",
        "                #On lit le contenu du fichier et on le stocke dans la liste \"texte\"\n",
        "                with open(input_file, 'r', encoding='utf-8') as file:\n",
        "                    texte=file.read()\n",
        "\n",
        "                    #Pour \"nb_paragraphes\", on ajoute +1 au compteur car il compte les retours à la ligne (et donc ne compte pas le premier paragrpahe)\n",
        "                    nb_paragraphes=texte.count('\\n')+1\n",
        "\n",
        "                    #Cette ligne est ajoutée après avoir remarqué le traitement littéral de '\\n' dans certains des fichiers.\n",
        "                    texte=texte.replace('\\n', ' ')\n",
        "\n",
        "                    #On segmente et tokénise la liste texte avec les fonctions segment et tokenize\n",
        "                    sentences=segment(texte)\n",
        "                    tokens_list=[]\n",
        "                    for sentence in sentences:\n",
        "                        tokens=tokenize(sentence)\n",
        "                        tokens_list.append(tokens)\n",
        "\n",
        "                    #On crée le fichier NAME.seg.json pour \"stocker\" le résultat de notre segmentation et tokénisation\n",
        "                    output_json_file=os.path.join(dir_name, f\"{filename}.seg.json\")\n",
        "                    with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
        "                        json.dump(tokens_list, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "                    #Dans le fichier stats.tsv, on écrit : nb_paragraphes, nb_phrases, nb_tokens pour chacun des fichiers dans un tableau\n",
        "                    nb_phrases=len(sentences)\n",
        "                    nb_tokens=sum(len(sentence) for sentence in tokens_list)\n",
        "                    nb_chars=len(texte)\n",
        "                    stats_file.write(f\"{filename}\\t{nb_paragraphes}\\t{nb_phrases}\\t{nb_tokens}\\t{nb_chars}\\n\")\n",
        "\n",
        "                    #On incrémente compteu dur des fichiers traités\n",
        "                    fichiers_traites+=1\n",
        "                    print(f\"Le fichier {filename} a été traité.\")\n",
        "\n",
        "    #L'\"output\" de cette fonction sera le nombre de fichiers traités\n",
        "    return fichiers_traites\n",
        "\n",
        "#On fait la fonction process_dir sur nos données importées \"/content/\" ici pour ce Google Colab\n",
        "process_dir(\"/content/\",\"stats.tsv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}