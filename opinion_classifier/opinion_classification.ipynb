{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lomjxb5wbov"
      },
      "source": [
        "#Projet : classifieur d'avis Google"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY4mHNkbykeP"
      },
      "source": [
        "##Description du projet\n",
        "\n",
        "Ce projet a pour but de classifier des avis Google de restaurants comme positif, n√©gatif ou neutre. Notre dataset est constitu√© d'avis r√©cup√©r√©s directement sur Google Maps et nettoy√©s avec pour chacun d'eux : une note donn√©e par l'utilisateur (nombre d‚Äô√©toiles), un label ‚Äúpositif‚Äù, ‚Äún√©gatif‚Äù ou ‚Äúneutre‚Äù et un texte d'√©valuation. Nous estimons :\n",
        "\n",
        "*   üôÅ 1-2 √©toiles : avis n√©gatif,\n",
        "*   üòê 3 √©toiles : avis neutre,\n",
        "*   üôÇ 4-5 √©toiles : avis positif.\n",
        "\n",
        "L'utilisation du mod√®le DistilBERT permet de classer un avis comme positif, n√©gatif ou neutre selon le texte contenu dans l‚Äôavis post√©.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUpHWj9pXU-V"
      },
      "source": [
        "##Importation de modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW0XsOc-XWtn"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver_manager\n",
        "!pip install parsel\n",
        "\n",
        "!wget https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\n",
        "!unzip chromedriver_linux64.zip -d webdriver\n",
        "!wget http://mirror.cs.uchicago.edu/google-chrome/pool/main/g/google-chrome-stable/google-chrome-stable_114.0.5735.106-1_amd64.deb\n",
        "!sudo apt install libu2f-udev libvulkan1\n",
        "!sudo apt --fix-broken install\n",
        "!dpkg -i google-chrome-stable_114.0.5735.106-1_amd64.deb\n",
        "\n",
        "!ls /usr/bin/google-chrome\n",
        "\n",
        "#Pour r√©cup√©rer nos donn√©es\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from urllib.parse import unquote\n",
        "\n",
        "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
        "from selenium.webdriver.common.by import By\n",
        "from parsel import Selector\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import json\n",
        "\n",
        "#Pour diviser nos donn√©es\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Pour entra√Æner et tester notre mod√®le\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Pour √©valuer notre mod√®le\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2vS7Cjo0kqG"
      },
      "source": [
        "##R√©cup√©ration des donn√©es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7yNrpVKYFD0"
      },
      "source": [
        "Nous r√©cup√©rons nos donn√©es en partant de plusieurs URLs avec la recherche \"nom_de_ville restaurant\" dans Google Maps.\n",
        "Notre fonction crawl permet de r√©cup√©rer d'autres liens √† partir de ces URLs : nous allons cliquer sur chaque lien pr√©sent dans la page (liste des restaurants), puis sur l'onglet \"Avis\" pour chaque lien.\n",
        "Notre fonction scrap r√©cup√®re le nombre d'√©toiles de l'avis, et le texte de l'avis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izPaj_xWZH09"
      },
      "source": [
        "###Installation du chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg6JqNPzYEos"
      },
      "outputs": [],
      "source": [
        "#installation du webdriver chrome\n",
        "#from selenium import webdriver\n",
        "#from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "service=ChromeService(executable_path='/content/webdriver/chromedriver')\n",
        "options=webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--start-maximized') #Ouverture dans une fen√™tre maximis√©e\n",
        "options.add_argument('--disable-infobars') #D√©sactive la barre d'√©tat\n",
        "options.add_argument('--disable-extensions')\n",
        "options.add_argument('--lang=fr')\n",
        "options.binary_location=\"/usr/bin/google-chrome\"\n",
        "\n",
        "driver=webdriver.Chrome(service=service,options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7UxPrguZJ3V"
      },
      "source": [
        "###Crawling et scrapping : r√©cup√©ration de donn√©es depuis Google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t57eam0RYmH9"
      },
      "outputs": [],
      "source": [
        "params={\n",
        "    'start_urls': [\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+grenoble/@45.1908097,5.7247407,17z?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+lyon/@45.7553741,4.8226548,14z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+marseille/@44.5172086,5.1139581,8z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+paris/@48.8618813,2.3325839,13z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+arcachon/@44.6524523,-1.1718973,13z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/New+York+restaurant/@40.7345967,-74.0158569,14z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/Los+Angeles+restaurant/@34.0252617,-118.3896428,12z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/Kansas+City+restaurant/@39.0748862,-94.6944134,13z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/Hyderabad+restaurant/@17.4131285,78.2432366,11z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/Londres+restaurant/@51.5179866,-0.169222,13z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/%C3%89dimbourg+restaurant/@55.9528971,-3.2056526,15z/data=!3m1!4b1?entry=ttu'}\n",
        "        ],\n",
        "    'url_pattern': r'https://www.google.fr/maps/search/',\n",
        "    'max_depth':1,\n",
        "    'max_crawled':100,\n",
        "    'latency':0.5,\n",
        "}\n",
        "\n",
        "\n",
        "def crawl(params):\n",
        "    start_urls=params['start_urls']\n",
        "    max_depth=params['max_depth']\n",
        "    max_crawled=params['max_crawled']\n",
        "\n",
        "    for start_url in params['start_urls']:\n",
        "      start_url['url']=unquote(start_url['url'])\n",
        "\n",
        "\n",
        "    if not start_urls:\n",
        "        print('No start URLs specified in actor input, exiting...')\n",
        "\n",
        "    default_queue=[]\n",
        "    crawled_data=[]\n",
        "    already_crawled={}\n",
        "    nb_crawled=0\n",
        "\n",
        "    for start_url in start_urls:\n",
        "        url=start_url.get('url')\n",
        "        print(f'Enqueuing {url} ...')\n",
        "        default_queue.append({\n",
        "            'url': url,\n",
        "            'userData': {'depth': 0},\n",
        "        })\n",
        "\n",
        "        print('Launching Chrome WebDriver...')\n",
        "\n",
        "        chrome_options=ChromeOptions()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "        driver=webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "\n",
        "        #On g√®re la page de cookies si y'en a une\n",
        "        try:\n",
        "          accept_button=driver.find_element(By.XPATH, '//button[contains(@aria-label, \"Accept all\")]')\n",
        "          accept_button.click()\n",
        "          print(\"Clicked on 'Accept all' button\")\n",
        "        except NoSuchElementException:\n",
        "          print(\"No 'Accept all' button found\")\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "        try:\n",
        "          driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'end', inline: 'nearest'});\", driver.find_element(By.CLASS_NAME, \"k7jAl miFGmb lJ3Kh PLbyfe\"))\n",
        "          print(\"Page scrolled\")\n",
        "        except NoSuchElementException:\n",
        "          print(\"Couldn't scroll\")\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "\n",
        "        while len(default_queue)>0:\n",
        "            request=default_queue.pop()\n",
        "            nb_crawled+=1\n",
        "            if nb_crawled>max_crawled:\n",
        "                print(f\"{max_crawled=} limit has been reached. Crawling is finished\")\n",
        "                break\n",
        "            time.sleep(params['latency'])\n",
        "            url=request['url']\n",
        "            depth=request['userData']['depth']\n",
        "\n",
        "            #On commence √† crawl depuis les URL de d√©part\n",
        "            print(f'Scraping {url} ...')\n",
        "\n",
        "\n",
        "            try:\n",
        "                driver.get(url)\n",
        "\n",
        "                if depth<max_depth:\n",
        "                  #on cherche ici √† r√©cup√©rer les liens sur la page (pages de resto)\n",
        "                  for link in driver.find_elements(By.XPATH, '//a[contains(@class, \"hfpxzc\")]'):\n",
        "                      link_href=link.get_attribute('href')\n",
        "                      link_url=urljoin(url, link_href)\n",
        "                      if link_url.startswith(('http://www.google.fr/', 'https://www.google.fr/','google.fr/','http://www.google.com/','https://www.google.com/')):\n",
        "                          if url not in already_crawled:\n",
        "                              print(f'Enqueuing {link_url} ...')\n",
        "                              default_queue.append({\n",
        "                                  'url': link_url,\n",
        "                                  'userData':{'depth': depth+1},\n",
        "                              })\n",
        "\n",
        "                #On r√©cup la page des avis des pages de resto\n",
        "                try:\n",
        "                    avis_button=driver.find_element(By.XPATH, '//button[contains(@aria-label, \"Reviews\")]')\n",
        "                    #avis_bouton=driver.find_element(By.XPATH, '//button[contains(@class,\"hh2c6 \")]')\n",
        "                    avis_button.click()\n",
        "                    #avis_bouton.click()\n",
        "                    print(\"Clicked on 'Reviews'\")\n",
        "\n",
        "                    time.sleep(3)\n",
        "\n",
        "                    reviews_url=driver.current_url\n",
        "\n",
        "                    if url!=start_url['url']:\n",
        "                        crawled_data.append({'restaurant_url': url, 'reviews_url': reviews_url})\n",
        "\n",
        "                except NoSuchElementException:\n",
        "                    print(\"No 'Reviews' button found\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f'Cannot extract data from {url}. Error: {e}')\n",
        "            finally:\n",
        "                already_crawled[url]=1\n",
        "\n",
        "        driver.quit()\n",
        "\n",
        "    return crawled_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xSKrhVkYt09"
      },
      "outputs": [],
      "source": [
        "restos=crawl(params)\n",
        "#for resto in restos:\n",
        "  #print(resto)\n",
        "\n",
        "reviews_urls=[resto['reviews_url'] for resto in restos]\n",
        "print(reviews_urls)\n",
        "len(reviews_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvI77K7mOjQL"
      },
      "outputs": [],
      "source": [
        "#print(restos)\n",
        "reviews_urls=[resto['reviews_url'] for resto in restos]\n",
        "print(reviews_urls)\n",
        "print(\"Nombres d'URLs r√©cup√©r√©es : \",len(reviews_urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S44ioFWYuPt"
      },
      "outputs": [],
      "source": [
        "def scrap(driver,url_list):\n",
        "    all_results=[]\n",
        "\n",
        "    print('Launching Chrome WebDriver...')\n",
        "\n",
        "    chrome_options=webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    print('Successfully launched')\n",
        "\n",
        "    #On initie un set ici pour ne pas avoir de reviews en double\n",
        "    seen_reviews = set()\n",
        "\n",
        "    for url in url_list:\n",
        "        driver.get(url)\n",
        "        print(f'Start scraping {url} ...')\n",
        "        try:\n",
        "            accept_button = driver.find_element(By.XPATH, '//button[contains(@aria-label, \"Accept all\")]')\n",
        "            accept_button.click()\n",
        "            time.sleep(2)\n",
        "            print(\"Clicked on 'Accept all' button\")\n",
        "        except NoSuchElementException:\n",
        "            print(\"No 'Accept all' button found\")\n",
        "\n",
        "        try:\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'end', inline: 'nearest'});\", driver.find_element(By.CLASS_NAME, \"bJzME\"))\n",
        "            print(\"Page scrolled\")\n",
        "            time.sleep(2)\n",
        "        except NoSuchElementException:\n",
        "            print(\"Couldn't scroll\")\n",
        "\n",
        "        page_content=driver.page_source\n",
        "        response=Selector(text=page_content)\n",
        "\n",
        "        results=[]\n",
        "\n",
        "        for el in response.xpath('//div[contains(@class, \"m6QErb \")]'):\n",
        "            try:\n",
        "                more_button=driver.find_element(By.XPATH, '//button[contains(@aria-label,\"See more\")]')\n",
        "                more_button.click()\n",
        "                print(\"Clicked on 'See more'\")\n",
        "                time.sleep(2)\n",
        "            except NoSuchElementException:\n",
        "                print(\"No 'See more' button found\")\n",
        "\n",
        "            rating_list=el.xpath('.//span[contains(@class, \"kvMYJc\")]/@aria-label').extract()\n",
        "            body_list=el.xpath('.//span[contains(@class, \"wiI7pd\")]/text()').extract()\n",
        "            if rating_list and body_list:\n",
        "                for rating,body in zip(rating_list,body_list):\n",
        "                    #ici on va v√©rifier si la review n'a pas d√©j√† √©t√© scrap pour √©viter les doublons\n",
        "                    review_key=f'{rating}-{body}'\n",
        "                    if review_key not in seen_reviews:\n",
        "                        results.append({\n",
        "                            'rating': rating.replace('stars', '').replace('star', '').strip(),\n",
        "                            'body': body.strip(),\n",
        "                        })\n",
        "                        seen_reviews.add(review_key)\n",
        "\n",
        "        all_results.extend(results)\n",
        "\n",
        "    print('Closing Chrome WebDriver...')\n",
        "    driver.quit()\n",
        "    print('Successfully closed')\n",
        "\n",
        "    print(\"Nombre d'avis r√©cup√©r√©s : \", len(all_results))\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6PMux8mY1nW"
      },
      "outputs": [],
      "source": [
        "scrapped_reviews=scrap(driver,reviews_urls)\n",
        "\n",
        "for result in scrapped_reviews:\n",
        "    print(f\"Note : {result['rating']}\")\n",
        "    print(f\"Texte : {result['body']}\")\n",
        "    print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxGRvpQzZD8d"
      },
      "source": [
        "###Division de nos donn√©es en partition train et test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUMQ0pb_ZDh1"
      },
      "outputs": [],
      "source": [
        "def split_train_test_data(data,test_size=0.2,random_state=None):\n",
        "    \"\"\"\n",
        "    Cette fonction divise les donn√©es en ensembles d'entra√Ænement et de test.\n",
        "\n",
        "    Entr√©e:\n",
        "        data (list): liste des donn√©es √† diviser\n",
        "        test_size (float): proportion des donn√©es √† inclure dans test\n",
        "        random_state (int): graine pour la g√©n√©ration de nombres al√©atoires pour la reproductibilit√©\n",
        "\n",
        "    Sortie:\n",
        "        tuple contenant les ensembles d'entra√Ænement et de test (X_train, X_test)\n",
        "    \"\"\"\n",
        "    X=[review['body'] for review in data]\n",
        "    y=[review['rating'] for review in data]\n",
        "\n",
        "    X_train, X_test,y_train,y_test=train_test_split(X,y,test_size=test_size,random_state=random_state)\n",
        "\n",
        "    print(\"Les donn√©es ont bien √©t√© divis√©es.\")\n",
        "    print(f\"Taille de l'ensemble d'entra√Ænement : {len(X_train)}\")\n",
        "    print(f\"Taille de l'ensemble de test : {len(X_test)}\")\n",
        "\n",
        "    return (X_train,X_test,y_train,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez9EJDmOZU3W"
      },
      "source": [
        "###Labellisation de nos donn√©es\n",
        "Pour chaque avis r√©cup√©r√©, on lui donne un label \"positif\", \"n√©gatif\" ou \"neutre\" selon son nombre d'√©toiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R54k_azPZe4t"
      },
      "outputs": [],
      "source": [
        "def label_reviews(reviews:list,ratings:list):\n",
        "    \"\"\"\n",
        "    Cette fonction va donner un label √† chaque review r√©cup√©r√©e :\n",
        "      - 1-2 √©toiles : avis n√©gatif,\n",
        "      - 3 √©toiles : avis neutre,\n",
        "      - 4-5 √©toiles : avis positif.\n",
        "\n",
        "    Entr√©es :\n",
        "      reviews : liste des reviews √† labelliser\n",
        "      ratings : liste des notes (1 √† 5) associ√©es aux reviews\n",
        "\n",
        "    Sortie :\n",
        "      les reviews labellis√©es\n",
        "\n",
        "    N√©cessite les modules suivants :\n",
        "    \"\"\"\n",
        "    labeled_reviews=[]\n",
        "    num_negatif=0\n",
        "    num_neutre=0\n",
        "    num_positif=0\n",
        "\n",
        "    for review,rating in zip(reviews,ratings):\n",
        "        stars=rating\n",
        "        if stars in ['1','2']:\n",
        "            label='negatif'\n",
        "            num_negatif+=1\n",
        "        elif stars=='3':\n",
        "            label='neutre'\n",
        "            num_neutre+=1\n",
        "        elif stars in ['4','5']:\n",
        "            label='positif'\n",
        "            num_positif+=1\n",
        "        else:\n",
        "            label='unknown'  #au cas o√π on a mal ou pas r√©cup le nb d'√©toiles\n",
        "\n",
        "        labeled_reviews.append({'rating': stars, 'label': label, 'body': review})\n",
        "\n",
        "    print(\"Nombre d'avis n√©gatifs :\",num_negatif)\n",
        "    print(\"Nombre d'avis neutres :\",num_neutre)\n",
        "    print(\"Nombre d'avis positifs :\",num_positif)\n",
        "    print(labeled_reviews)\n",
        "\n",
        "    return labeled_reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZRZYl_Z4Btl"
      },
      "source": [
        "##Entra√Ænement du mod√®le"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bf3q0QdZzvV"
      },
      "source": [
        "###Chargement du mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSF7Q2XeZhD1"
      },
      "outputs": [],
      "source": [
        "#On charge le mod√®le et le tokenizer DistilBERT\n",
        "modele_nom='distilbert-base-uncased'\n",
        "tokenizer=DistilBertTokenizer.from_pretrained(modele_nom)\n",
        "modele=DistilBertForSequenceClassification.from_pretrained(modele_nom,num_labels=3)  #3 classes : positif, neutre, n√©gatif"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtSfW52zZ21G"
      },
      "source": [
        "###Pr√©traitement des donn√©es : tokenisation et encodage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaCYLbH1Zj2l"
      },
      "outputs": [],
      "source": [
        "def tokenize_reviews(reviews:list):\n",
        "    \"\"\"\n",
        "    Cette fonction tokenise les avis et pr√©pare les masques d'attention.\n",
        "\n",
        "    Entr√©es:\n",
        "        reviews : Liste des avis √† tokenizer\n",
        "        labels : Liste des √©tiquettes associ√©es aux avis\n",
        "\n",
        "    Sortie:\n",
        "        Les entr√©es, les masques d'attention et les √©tiquettes d'entra√Ænement et de test encod√©s.\n",
        "    \"\"\"\n",
        "\n",
        "    #Pour r√©cup les identifiants des tokens d'entr√©e\n",
        "    input_ids=[]\n",
        "    #Pour stocker les masques d'attention\n",
        "    attention_masks=[]\n",
        "    #Pour stocker les √©tiquettes\n",
        "    labels=[]\n",
        "\n",
        "    #Sur chaque avis, on fait une boucle\n",
        "    for review in tqdm(reviews):\n",
        "\n",
        "        body=review['body']  #On r√©cup le corps de l'avis\n",
        "        label=review['label']  #On r√©cup l'√©tiquette de l'avis\n",
        "\n",
        "        encoded_dict=tokenizer.encode_plus( #On encode l'avis avec le tokenizer\n",
        "            body,\n",
        "            add_special_tokens=True, #Ajout des tokens sp√©ciaux [CLS] (d√©but de la s√©quence) et [SEP] (fin de s√©quence)\n",
        "            max_length=128,  #Taille max des s√©quences\n",
        "            padding='max_length', #Remplissage des s√©quences √† la taille max\n",
        "            truncation=True, #Si la s√©quence d√©passe la taille max, on la tronque\n",
        "            return_attention_mask=True, #On active masque d'attention\n",
        "            return_tensors='pt' #Retour des tensors Pytorch\n",
        "        )\n",
        "\n",
        "        #Pour chaque review, on ajoute les identifiants des tokens d'entr√©e √† la liste et les masques d'attention √©galement\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        #On ajoute l'√©tiquette\n",
        "        labels.append(label)\n",
        "\n",
        "    #On concat√®ne les identifiants, puis les masques d'attention en un seul tensor\n",
        "    input_ids=torch.cat(input_ids, dim=0)\n",
        "    attention_masks=torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    #Ici on convertit les √©tiquettes textuelles en identifiants num√©riques en utilisant le dictionnaire\n",
        "    label_dict={\"positif\":0,\"negatif\":1,\"neutre\":2}\n",
        "    labels=[label_dict[label] for label in labels]\n",
        "    #On convertit nos √©tiquettes en tensor PyTorch\n",
        "    labels=torch.tensor(labels)\n",
        "\n",
        "\n",
        "    return input_ids,attention_masks,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVuT5CWZhii5"
      },
      "outputs": [],
      "source": [
        "#Utilisation de la fonction pour diviser les donn√©es\n",
        "X_train,X_test,y_train,y_test=split_train_test_data(scrapped_reviews)\n",
        "\n",
        "#√âtiquetage des donn√©es d'entra√Ænement et de test\n",
        "labeled_train_reviews=label_reviews(X_train,y_train)\n",
        "labeled_test_reviews=label_reviews(X_test,y_test)\n",
        "\n",
        "#Utilisation des donn√©es √©tiquet√©es pour la tokenization et encodage\n",
        "train_inputs,train_masks,train_labels=tokenize_reviews(labeled_train_reviews)\n",
        "test_inputs,test_masks,test_labels=tokenize_reviews(labeled_test_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz3qmI_jZ8Je"
      },
      "source": [
        "###Entra√Ænement du mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "205fn43eZl51"
      },
      "outputs": [],
      "source": [
        "def train_model(model,train_dataloader,optimizer,epochs=3):\n",
        "    \"\"\"\n",
        "    La fonction train_model entra√Æne notre mod√®le sur les donn√©es d'entra√Ænement que nous avons r√©cup√©r√©es.\n",
        "\n",
        "    Entr√©es :\n",
        "      model : le mod√®le √† entra√Æner\n",
        "      train_dataloader : le dataloader contenant nos donn√©es d'entra√Ænement\n",
        "      optimizer : l'optimiseur qui est utilis√© pour mettre √† jour les poids du mod√®le\n",
        "      epochs : le nombre d'√©poques pour lesquelles entra√Æner le mod√®le\n",
        "\n",
        "    Sortie :\n",
        "      end_train, cha√Æne de caract√®res indiquant l'√©tat de l'entra√Ænement\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        #On v√©rifie si le GPU est dispo (plus rapide), sinon on utilise le CPU\n",
        "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        #On d√©place le mod√®le sur le GPU ou le CPU\n",
        "        model.to(device)\n",
        "\n",
        "        #On boucle sur le nombre d'√©poques donn√© en entr√©e :\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            #On met le mod√®le en mode \"entra√Ænement\"\n",
        "            model.train()\n",
        "\n",
        "            #Ici on initialise la perte totale pour l'√©poque en cours\n",
        "            perte_totale=0\n",
        "\n",
        "            #On boucle sur les lots dans le dataloader d'entra√Ænement\n",
        "            for batch in tqdm(train_dataloader):\n",
        "                b_input_ids=batch[0].to(device)\n",
        "                b_input_mask=batch[1].to(device)\n",
        "                b_labels=batch[2].to(device)\n",
        "\n",
        "                #R√©initialisation des gradients du mod√®le\n",
        "                model.zero_grad()\n",
        "\n",
        "                #On passe les donn√©es au mod√®le pour obtenir des sorties\n",
        "                outputs=model(b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n",
        "                loss=outputs.loss #On obtient ici la perte √† partir des sorties obtenues\n",
        "\n",
        "                #Ajout de cette perte √† la perte totale\n",
        "                perte_totale+=loss.item()\n",
        "\n",
        "                #On calcule les gradients\n",
        "                loss.backward()\n",
        "\n",
        "                #On met √† jour le poids du mod√®le avec l'optimiseur\n",
        "                optimizer.step()\n",
        "\n",
        "            #Retour sur la perte totale pour cette √©poque\n",
        "            print(f'Epoque : {epoch+1}/{epochs}, Perte : {perte_totale}')\n",
        "\n",
        "        end_train=\"Entra√Ænement termin√© !\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "      end_train=f\"Entra√Ænement impossible : {str(e)}\"\n",
        "\n",
        "    return end_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofwlj9M7ZoK9"
      },
      "outputs": [],
      "source": [
        "#On cr√©e les dataloaders\n",
        "train_data=TensorDataset(train_inputs,train_masks,train_labels)\n",
        "train_dataloader=DataLoader(train_data,batch_size=32,shuffle=True)\n",
        "\n",
        "#Optimiseur\n",
        "optimizer=torch.optim.AdamW(modele.parameters(),lr=2e-5)\n",
        "\n",
        "#Entra√Ænement du mod√®le\n",
        "train_model(modele,train_dataloader,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY-QVpJt0prX"
      },
      "source": [
        "##Test du mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynXWvxO_ZpZ9"
      },
      "outputs": [],
      "source": [
        "def test_model(model,dataloader):\n",
        "    \"\"\"\n",
        "    Cette fonction teste notre mod√®le sur des donn√©es test.\n",
        "\n",
        "    Entr√©es :\n",
        "      model : le mod√®le √† tester\n",
        "      dataloader : le dataloader contenant les donn√©es de test\n",
        "\n",
        "    Sortie :\n",
        "      les √©tiquettes pr√©dites par notre mod√®le\n",
        "    \"\"\"\n",
        "    try:\n",
        "        #On v√©rifie si le GPU est dispo (plus rapide), sinon on utilise le CPU\n",
        "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        #On bouge le mod√®le sur le GPU ou CPU\n",
        "        model.to(device)\n",
        "\n",
        "        #On met le mod√®le en mode \"eval\" (pas de maj des gradients)\n",
        "        model.eval()\n",
        "\n",
        "        #Pour stocker les pr√©dictions faites par notre mod√®le\n",
        "        predictions=[]\n",
        "\n",
        "        #On boucle sur les lots des donn√©es dans le dataloader test\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                inputs=batch[0].to(device)\n",
        "                masks=batch[1].to(device)\n",
        "\n",
        "                #On passe les donn√©es au mod√®le pour obtenir des logits\n",
        "                outputs=model(inputs,attention_mask=masks)\n",
        "                logits=outputs.logits\n",
        "                _,predicted_labels=torch.max(logits, 1) #fct argmax pour obtenir √©tiquettes pr√©dites\n",
        "\n",
        "                #On ajoute les labels pr√©dits √† notre liste\n",
        "                predictions.extend(predicted_labels.tolist())\n",
        "\n",
        "        #print(\"Indices avant conversion : \",predictions)\n",
        "\n",
        "        try:\n",
        "          #Ici on convertit les indices pr√©dits (0, 1, 2) en √©tiquettes (positif, n√©gatif et neutre)\n",
        "          label_dict={\"positif\":0,\"negatif\":1,\"neutre\":2}\n",
        "          label_map_inverse={v: k for k, v in label_dict.items()} #on inverse le dico\n",
        "          predicted_labels=[label_map_inverse[prediction] for prediction in predictions]\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"La conversion des indices en labels est impossible : \",str(e))\n",
        "\n",
        "\n",
        "        end_test=\"Test termin√© !\"\n",
        "        #print(\"Labels pr√©dits : \",predicted_labels)\n",
        "\n",
        "        count_positif=predicted_labels.count(\"positif\")\n",
        "        count_negatif=predicted_labels.count(\"negatif\")\n",
        "        count_neutre=predicted_labels.count(\"neutre\")\n",
        "\n",
        "    except Exception as e:\n",
        "        end_test=f\"Test impossible : {str(e)}\"\n",
        "        predicted_labels=0\n",
        "        count_positif=0\n",
        "        count_negatif=0\n",
        "        count_neutre=0\n",
        "\n",
        "    print(end_test)\n",
        "    print(\"Avis positifs pr√©dits :\",count_positif)\n",
        "    print(\"Avis n√©gatifs pr√©dits :\",count_negatif)\n",
        "    print(\"Avis neutres pr√©dits :\",count_neutre)\n",
        "\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATWJniufZuuF"
      },
      "outputs": [],
      "source": [
        "#On cr√©e un dataloader pour les donn√©es de test\n",
        "test_data=TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_dataloader=DataLoader(test_data, batch_size=32)\n",
        "\n",
        "#Utilisation de ce dataloader pour tester le mod√®le\n",
        "test_predictions=test_model(modele,test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNHZ6IQ30tB_"
      },
      "source": [
        "##Evaluation du mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmqHVkDDZr89"
      },
      "outputs": [],
      "source": [
        "def evaluate(true_labels:list,predicted_labels:list):\n",
        "    \"\"\"\n",
        "    Cette fonction √©value les performances du mod√®le sur ses pr√©dictions par rapport aux vraies √©tiquettes.\n",
        "\n",
        "    Entr√©es :\n",
        "      true_labels : vraies √©tiquettes des donn√©es\n",
        "      predicted_labels : √©tiquettes pr√©dites par le mod√®le\n",
        "\n",
        "    Sortie :\n",
        "      les m√©triques : pr√©cision, rappel et Fmesure\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Calcul de la pr√©cision, du rappel et de la Fmesure\n",
        "      precision=precision_score(true_labels,predicted_labels, average='weighted',zero_division=0)\n",
        "      rappel=recall_score(true_labels,predicted_labels,average='weighted',zero_division=0)\n",
        "      fmesure=f1_score(true_labels,predicted_labels,average='weighted',zero_division=0)\n",
        "\n",
        "      #print(true_labels)\n",
        "      #print(predicted_labels)\n",
        "\n",
        "      count_positif_pred=predicted_labels.count(\"positif\")\n",
        "      count_negatif_pred=predicted_labels.count(\"negatif\")\n",
        "      count_neutre_pred=predicted_labels.count(\"neutre\")\n",
        "      count_positif_true=true_labels.count(\"positif\")\n",
        "      count_negatif_true=true_labels.count(\"negatif\")\n",
        "      count_neutre_true=true_labels.count(\"neutre\")\n",
        "\n",
        "      print(\"PREDICTIONS :\")\n",
        "      print(\"\\tAvis positifs pr√©dits :\",count_positif_pred)\n",
        "      print(\"\\tAvis n√©gatifs pr√©dits :\",count_negatif_pred)\n",
        "      print(\"\\tAvis neutres pr√©dits :\",count_neutre_pred)\n",
        "      print(\"\\tTOTAL : \",len(predicted_labels),\"\\n\")\n",
        "      print(\"=\"*50)\n",
        "      print(\"LABELS REELS :\")\n",
        "      print(\"\\tAvis positifs r√©els :\",count_positif_true)\n",
        "      print(\"\\tAvis n√©gatifs r√©els :\",count_negatif_true)\n",
        "      print(\"\\tAvis neutres r√©els :\",count_neutre_true)\n",
        "      print(\"\\tTOTAL : \",len(true_labels),\"\\n\")\n",
        "      print(\"=\"*50)\n",
        "\n",
        "      metriques=print(f\"EVALUATION DU MODELE :\\n\\tPr√©cision : {precision}\\n\\tRappel : {rappel}\\n\\tFmesure : {fmesure}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "      metriques=f\"Evaluation impossible : {str(e)}\"\n",
        "\n",
        "    return metriques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_Oorx-iZvU9"
      },
      "outputs": [],
      "source": [
        "true_labels=[review['label'] for review in labeled_test_reviews]\n",
        "evaluate(true_labels,test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Oj2xJ3ZwsO"
      },
      "outputs": [],
      "source": [
        "print(true_labels)\n",
        "print(test_predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}