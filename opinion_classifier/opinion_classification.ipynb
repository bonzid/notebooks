{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lomjxb5wbov"
      },
      "source": [
        "#Projet : classifieur d'avis Google"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY4mHNkbykeP"
      },
      "source": [
        "##Description du projet\n",
        "\n",
        "Ce projet a pour but de classifier des avis Google de restaurants comme positif, négatif ou neutre. Notre dataset est constitué d'avis récupérés directement sur Google Maps et nettoyés avec pour chacun d'eux : une note donnée par l'utilisateur (nombre d’étoiles), un label “positif”, “négatif” ou “neutre” et un texte d'évaluation. Nous estimons :\n",
        "\n",
        "*   🙁 1-2 étoiles : avis négatif,\n",
        "*   😐 3 étoiles : avis neutre,\n",
        "*   🙂 4-5 étoiles : avis positif.\n",
        "\n",
        "L'utilisation du modèle DistilBERT permet de classer un avis comme positif, négatif ou neutre selon le texte contenu dans l’avis posté.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUpHWj9pXU-V"
      },
      "source": [
        "##Importation de modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW0XsOc-XWtn"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver_manager\n",
        "!pip install parsel\n",
        "\n",
        "!wget https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\n",
        "!unzip chromedriver_linux64.zip -d webdriver\n",
        "!wget http://mirror.cs.uchicago.edu/google-chrome/pool/main/g/google-chrome-stable/google-chrome-stable_114.0.5735.106-1_amd64.deb\n",
        "!sudo apt install libu2f-udev libvulkan1\n",
        "!sudo apt --fix-broken install\n",
        "!dpkg -i google-chrome-stable_114.0.5735.106-1_amd64.deb\n",
        "\n",
        "!ls /usr/bin/google-chrome\n",
        "\n",
        "#Pour récupérer nos données\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from urllib.parse import unquote\n",
        "\n",
        "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
        "from selenium.webdriver.common.by import By\n",
        "from parsel import Selector\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import json\n",
        "\n",
        "#Pour diviser nos données\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Pour entraîner et tester notre modèle\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Pour évaluer notre modèle\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2vS7Cjo0kqG"
      },
      "source": [
        "##Récupération des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7yNrpVKYFD0"
      },
      "source": [
        "Nous récupérons nos données en partant de plusieurs URLs avec la recherche \"nom_de_ville restaurant\" dans Google Maps.\n",
        "Notre fonction crawl permet de récupérer d'autres liens à partir de ces URLs : nous allons cliquer sur chaque lien présent dans la page (liste des restaurants), puis sur l'onglet \"Avis\" pour chaque lien.\n",
        "Notre fonction scrap récupère le nombre d'étoiles de l'avis, et le texte de l'avis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izPaj_xWZH09"
      },
      "source": [
        "###Installation du chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg6JqNPzYEos"
      },
      "outputs": [],
      "source": [
        "#installation du webdriver chrome\n",
        "#from selenium import webdriver\n",
        "#from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "service=ChromeService(executable_path='/content/webdriver/chromedriver')\n",
        "options=webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--start-maximized') #Ouverture dans une fenêtre maximisée\n",
        "options.add_argument('--disable-infobars') #Désactive la barre d'état\n",
        "options.add_argument('--disable-extensions')\n",
        "options.add_argument('--lang=fr')\n",
        "options.binary_location=\"/usr/bin/google-chrome\"\n",
        "\n",
        "driver=webdriver.Chrome(service=service,options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7UxPrguZJ3V"
      },
      "source": [
        "###Crawling et scrapping : récupération de données depuis Google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t57eam0RYmH9"
      },
      "outputs": [],
      "source": [
        "params={\n",
        "    'start_urls': [\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+grenoble/@45.1908097,5.7247407,17z?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+lyon/@45.7553741,4.8226548,14z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+marseille/@44.5172086,5.1139581,8z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+paris/@48.8618813,2.3325839,13z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/restaurant+arcachon/@44.6524523,-1.1718973,13z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/New+York+restaurant/@40.7345967,-74.0158569,14z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/Los+Angeles+restaurant/@34.0252617,-118.3896428,12z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/Kansas+City+restaurant/@39.0748862,-94.6944134,13z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/Hyderabad+restaurant/@17.4131285,78.2432366,11z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/Londres+restaurant/@51.5179866,-0.169222,13z/data=!3m1!4b1?entry=ttu'},\n",
        "        {'url': 'https://www.google.fr/maps/search/%C3%89dimbourg+restaurant/@55.9528971,-3.2056526,15z/data=!3m1!4b1?entry=ttu'}\n",
        "        ],\n",
        "    'url_pattern': r'https://www.google.fr/maps/search/',\n",
        "    'max_depth':1,\n",
        "    'max_crawled':100,\n",
        "    'latency':0.5,\n",
        "}\n",
        "\n",
        "\n",
        "def crawl(params):\n",
        "    start_urls=params['start_urls']\n",
        "    max_depth=params['max_depth']\n",
        "    max_crawled=params['max_crawled']\n",
        "\n",
        "    for start_url in params['start_urls']:\n",
        "      start_url['url']=unquote(start_url['url'])\n",
        "\n",
        "\n",
        "    if not start_urls:\n",
        "        print('No start URLs specified in actor input, exiting...')\n",
        "\n",
        "    default_queue=[]\n",
        "    crawled_data=[]\n",
        "    already_crawled={}\n",
        "    nb_crawled=0\n",
        "\n",
        "    for start_url in start_urls:\n",
        "        url=start_url.get('url')\n",
        "        print(f'Enqueuing {url} ...')\n",
        "        default_queue.append({\n",
        "            'url': url,\n",
        "            'userData': {'depth': 0},\n",
        "        })\n",
        "\n",
        "        print('Launching Chrome WebDriver...')\n",
        "\n",
        "        chrome_options=ChromeOptions()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "        driver=webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "\n",
        "        #On gère la page de cookies si y'en a une\n",
        "        try:\n",
        "          accept_button=driver.find_element(By.XPATH, '//button[contains(@aria-label, \"Accept all\")]')\n",
        "          accept_button.click()\n",
        "          print(\"Clicked on 'Accept all' button\")\n",
        "        except NoSuchElementException:\n",
        "          print(\"No 'Accept all' button found\")\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "        try:\n",
        "          driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'end', inline: 'nearest'});\", driver.find_element(By.CLASS_NAME, \"k7jAl miFGmb lJ3Kh PLbyfe\"))\n",
        "          print(\"Page scrolled\")\n",
        "        except NoSuchElementException:\n",
        "          print(\"Couldn't scroll\")\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "\n",
        "        while len(default_queue)>0:\n",
        "            request=default_queue.pop()\n",
        "            nb_crawled+=1\n",
        "            if nb_crawled>max_crawled:\n",
        "                print(f\"{max_crawled=} limit has been reached. Crawling is finished\")\n",
        "                break\n",
        "            time.sleep(params['latency'])\n",
        "            url=request['url']\n",
        "            depth=request['userData']['depth']\n",
        "\n",
        "            #On commence à crawl depuis les URL de départ\n",
        "            print(f'Scraping {url} ...')\n",
        "\n",
        "\n",
        "            try:\n",
        "                driver.get(url)\n",
        "\n",
        "                if depth<max_depth:\n",
        "                  #on cherche ici à récupérer les liens sur la page (pages de resto)\n",
        "                  for link in driver.find_elements(By.XPATH, '//a[contains(@class, \"hfpxzc\")]'):\n",
        "                      link_href=link.get_attribute('href')\n",
        "                      link_url=urljoin(url, link_href)\n",
        "                      if link_url.startswith(('http://www.google.fr/', 'https://www.google.fr/','google.fr/','http://www.google.com/','https://www.google.com/')):\n",
        "                          if url not in already_crawled:\n",
        "                              print(f'Enqueuing {link_url} ...')\n",
        "                              default_queue.append({\n",
        "                                  'url': link_url,\n",
        "                                  'userData':{'depth': depth+1},\n",
        "                              })\n",
        "\n",
        "                #On récup la page des avis des pages de resto\n",
        "                try:\n",
        "                    avis_button=driver.find_element(By.XPATH, '//button[contains(@aria-label, \"Reviews\")]')\n",
        "                    #avis_bouton=driver.find_element(By.XPATH, '//button[contains(@class,\"hh2c6 \")]')\n",
        "                    avis_button.click()\n",
        "                    #avis_bouton.click()\n",
        "                    print(\"Clicked on 'Reviews'\")\n",
        "\n",
        "                    time.sleep(3)\n",
        "\n",
        "                    reviews_url=driver.current_url\n",
        "\n",
        "                    if url!=start_url['url']:\n",
        "                        crawled_data.append({'restaurant_url': url, 'reviews_url': reviews_url})\n",
        "\n",
        "                except NoSuchElementException:\n",
        "                    print(\"No 'Reviews' button found\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f'Cannot extract data from {url}. Error: {e}')\n",
        "            finally:\n",
        "                already_crawled[url]=1\n",
        "\n",
        "        driver.quit()\n",
        "\n",
        "    return crawled_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xSKrhVkYt09"
      },
      "outputs": [],
      "source": [
        "restos=crawl(params)\n",
        "#for resto in restos:\n",
        "  #print(resto)\n",
        "\n",
        "reviews_urls=[resto['reviews_url'] for resto in restos]\n",
        "print(reviews_urls)\n",
        "len(reviews_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvI77K7mOjQL"
      },
      "outputs": [],
      "source": [
        "#print(restos)\n",
        "reviews_urls=[resto['reviews_url'] for resto in restos]\n",
        "print(reviews_urls)\n",
        "print(\"Nombres d'URLs récupérées : \",len(reviews_urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S44ioFWYuPt"
      },
      "outputs": [],
      "source": [
        "def scrap(driver,url_list):\n",
        "    all_results=[]\n",
        "\n",
        "    print('Launching Chrome WebDriver...')\n",
        "\n",
        "    chrome_options=webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    print('Successfully launched')\n",
        "\n",
        "    #On initie un set ici pour ne pas avoir de reviews en double\n",
        "    seen_reviews = set()\n",
        "\n",
        "    for url in url_list:\n",
        "        driver.get(url)\n",
        "        print(f'Start scraping {url} ...')\n",
        "        try:\n",
        "            accept_button = driver.find_element(By.XPATH, '//button[contains(@aria-label, \"Accept all\")]')\n",
        "            accept_button.click()\n",
        "            time.sleep(2)\n",
        "            print(\"Clicked on 'Accept all' button\")\n",
        "        except NoSuchElementException:\n",
        "            print(\"No 'Accept all' button found\")\n",
        "\n",
        "        try:\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'end', inline: 'nearest'});\", driver.find_element(By.CLASS_NAME, \"bJzME\"))\n",
        "            print(\"Page scrolled\")\n",
        "            time.sleep(2)\n",
        "        except NoSuchElementException:\n",
        "            print(\"Couldn't scroll\")\n",
        "\n",
        "        page_content=driver.page_source\n",
        "        response=Selector(text=page_content)\n",
        "\n",
        "        results=[]\n",
        "\n",
        "        for el in response.xpath('//div[contains(@class, \"m6QErb \")]'):\n",
        "            try:\n",
        "                more_button=driver.find_element(By.XPATH, '//button[contains(@aria-label,\"See more\")]')\n",
        "                more_button.click()\n",
        "                print(\"Clicked on 'See more'\")\n",
        "                time.sleep(2)\n",
        "            except NoSuchElementException:\n",
        "                print(\"No 'See more' button found\")\n",
        "\n",
        "            rating_list=el.xpath('.//span[contains(@class, \"kvMYJc\")]/@aria-label').extract()\n",
        "            body_list=el.xpath('.//span[contains(@class, \"wiI7pd\")]/text()').extract()\n",
        "            if rating_list and body_list:\n",
        "                for rating,body in zip(rating_list,body_list):\n",
        "                    #ici on va vérifier si la review n'a pas déjà été scrap pour éviter les doublons\n",
        "                    review_key=f'{rating}-{body}'\n",
        "                    if review_key not in seen_reviews:\n",
        "                        results.append({\n",
        "                            'rating': rating.replace('stars', '').replace('star', '').strip(),\n",
        "                            'body': body.strip(),\n",
        "                        })\n",
        "                        seen_reviews.add(review_key)\n",
        "\n",
        "        all_results.extend(results)\n",
        "\n",
        "    print('Closing Chrome WebDriver...')\n",
        "    driver.quit()\n",
        "    print('Successfully closed')\n",
        "\n",
        "    print(\"Nombre d'avis récupérés : \", len(all_results))\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6PMux8mY1nW"
      },
      "outputs": [],
      "source": [
        "scrapped_reviews=scrap(driver,reviews_urls)\n",
        "\n",
        "for result in scrapped_reviews:\n",
        "    print(f\"Note : {result['rating']}\")\n",
        "    print(f\"Texte : {result['body']}\")\n",
        "    print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxGRvpQzZD8d"
      },
      "source": [
        "###Division de nos données en partition train et test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUMQ0pb_ZDh1"
      },
      "outputs": [],
      "source": [
        "def split_train_test_data(data,test_size=0.2,random_state=None):\n",
        "    \"\"\"\n",
        "    Cette fonction divise les données en ensembles d'entraînement et de test.\n",
        "\n",
        "    Entrée:\n",
        "        data (list): liste des données à diviser\n",
        "        test_size (float): proportion des données à inclure dans test\n",
        "        random_state (int): graine pour la génération de nombres aléatoires pour la reproductibilité\n",
        "\n",
        "    Sortie:\n",
        "        tuple contenant les ensembles d'entraînement et de test (X_train, X_test)\n",
        "    \"\"\"\n",
        "    X=[review['body'] for review in data]\n",
        "    y=[review['rating'] for review in data]\n",
        "\n",
        "    X_train, X_test,y_train,y_test=train_test_split(X,y,test_size=test_size,random_state=random_state)\n",
        "\n",
        "    print(\"Les données ont bien été divisées.\")\n",
        "    print(f\"Taille de l'ensemble d'entraînement : {len(X_train)}\")\n",
        "    print(f\"Taille de l'ensemble de test : {len(X_test)}\")\n",
        "\n",
        "    return (X_train,X_test,y_train,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez9EJDmOZU3W"
      },
      "source": [
        "###Labellisation de nos données\n",
        "Pour chaque avis récupéré, on lui donne un label \"positif\", \"négatif\" ou \"neutre\" selon son nombre d'étoiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R54k_azPZe4t"
      },
      "outputs": [],
      "source": [
        "def label_reviews(reviews:list,ratings:list):\n",
        "    \"\"\"\n",
        "    Cette fonction va donner un label à chaque review récupérée :\n",
        "      - 1-2 étoiles : avis négatif,\n",
        "      - 3 étoiles : avis neutre,\n",
        "      - 4-5 étoiles : avis positif.\n",
        "\n",
        "    Entrées :\n",
        "      reviews : liste des reviews à labelliser\n",
        "      ratings : liste des notes (1 à 5) associées aux reviews\n",
        "\n",
        "    Sortie :\n",
        "      les reviews labellisées\n",
        "\n",
        "    Nécessite les modules suivants :\n",
        "    \"\"\"\n",
        "    labeled_reviews=[]\n",
        "    num_negatif=0\n",
        "    num_neutre=0\n",
        "    num_positif=0\n",
        "\n",
        "    for review,rating in zip(reviews,ratings):\n",
        "        stars=rating\n",
        "        if stars in ['1','2']:\n",
        "            label='negatif'\n",
        "            num_negatif+=1\n",
        "        elif stars=='3':\n",
        "            label='neutre'\n",
        "            num_neutre+=1\n",
        "        elif stars in ['4','5']:\n",
        "            label='positif'\n",
        "            num_positif+=1\n",
        "        else:\n",
        "            label='unknown'  #au cas où on a mal ou pas récup le nb d'étoiles\n",
        "\n",
        "        labeled_reviews.append({'rating': stars, 'label': label, 'body': review})\n",
        "\n",
        "    print(\"Nombre d'avis négatifs :\",num_negatif)\n",
        "    print(\"Nombre d'avis neutres :\",num_neutre)\n",
        "    print(\"Nombre d'avis positifs :\",num_positif)\n",
        "    print(labeled_reviews)\n",
        "\n",
        "    return labeled_reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZRZYl_Z4Btl"
      },
      "source": [
        "##Entraînement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bf3q0QdZzvV"
      },
      "source": [
        "###Chargement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSF7Q2XeZhD1"
      },
      "outputs": [],
      "source": [
        "#On charge le modèle et le tokenizer DistilBERT\n",
        "modele_nom='distilbert-base-uncased'\n",
        "tokenizer=DistilBertTokenizer.from_pretrained(modele_nom)\n",
        "modele=DistilBertForSequenceClassification.from_pretrained(modele_nom,num_labels=3)  #3 classes : positif, neutre, négatif"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtSfW52zZ21G"
      },
      "source": [
        "###Prétraitement des données : tokenisation et encodage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaCYLbH1Zj2l"
      },
      "outputs": [],
      "source": [
        "def tokenize_reviews(reviews:list):\n",
        "    \"\"\"\n",
        "    Cette fonction tokenise les avis et prépare les masques d'attention.\n",
        "\n",
        "    Entrées:\n",
        "        reviews : Liste des avis à tokenizer\n",
        "        labels : Liste des étiquettes associées aux avis\n",
        "\n",
        "    Sortie:\n",
        "        Les entrées, les masques d'attention et les étiquettes d'entraînement et de test encodés.\n",
        "    \"\"\"\n",
        "\n",
        "    #Pour récup les identifiants des tokens d'entrée\n",
        "    input_ids=[]\n",
        "    #Pour stocker les masques d'attention\n",
        "    attention_masks=[]\n",
        "    #Pour stocker les étiquettes\n",
        "    labels=[]\n",
        "\n",
        "    #Sur chaque avis, on fait une boucle\n",
        "    for review in tqdm(reviews):\n",
        "\n",
        "        body=review['body']  #On récup le corps de l'avis\n",
        "        label=review['label']  #On récup l'étiquette de l'avis\n",
        "\n",
        "        encoded_dict=tokenizer.encode_plus( #On encode l'avis avec le tokenizer\n",
        "            body,\n",
        "            add_special_tokens=True, #Ajout des tokens spéciaux [CLS] (début de la séquence) et [SEP] (fin de séquence)\n",
        "            max_length=128,  #Taille max des séquences\n",
        "            padding='max_length', #Remplissage des séquences à la taille max\n",
        "            truncation=True, #Si la séquence dépasse la taille max, on la tronque\n",
        "            return_attention_mask=True, #On active masque d'attention\n",
        "            return_tensors='pt' #Retour des tensors Pytorch\n",
        "        )\n",
        "\n",
        "        #Pour chaque review, on ajoute les identifiants des tokens d'entrée à la liste et les masques d'attention également\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        #On ajoute l'étiquette\n",
        "        labels.append(label)\n",
        "\n",
        "    #On concatène les identifiants, puis les masques d'attention en un seul tensor\n",
        "    input_ids=torch.cat(input_ids, dim=0)\n",
        "    attention_masks=torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    #Ici on convertit les étiquettes textuelles en identifiants numériques en utilisant le dictionnaire\n",
        "    label_dict={\"positif\":0,\"negatif\":1,\"neutre\":2}\n",
        "    labels=[label_dict[label] for label in labels]\n",
        "    #On convertit nos étiquettes en tensor PyTorch\n",
        "    labels=torch.tensor(labels)\n",
        "\n",
        "\n",
        "    return input_ids,attention_masks,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVuT5CWZhii5"
      },
      "outputs": [],
      "source": [
        "#Utilisation de la fonction pour diviser les données\n",
        "X_train,X_test,y_train,y_test=split_train_test_data(scrapped_reviews)\n",
        "\n",
        "#Étiquetage des données d'entraînement et de test\n",
        "labeled_train_reviews=label_reviews(X_train,y_train)\n",
        "labeled_test_reviews=label_reviews(X_test,y_test)\n",
        "\n",
        "#Utilisation des données étiquetées pour la tokenization et encodage\n",
        "train_inputs,train_masks,train_labels=tokenize_reviews(labeled_train_reviews)\n",
        "test_inputs,test_masks,test_labels=tokenize_reviews(labeled_test_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz3qmI_jZ8Je"
      },
      "source": [
        "###Entraînement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "205fn43eZl51"
      },
      "outputs": [],
      "source": [
        "def train_model(model,train_dataloader,optimizer,epochs=3):\n",
        "    \"\"\"\n",
        "    La fonction train_model entraîne notre modèle sur les données d'entraînement que nous avons récupérées.\n",
        "\n",
        "    Entrées :\n",
        "      model : le modèle à entraîner\n",
        "      train_dataloader : le dataloader contenant nos données d'entraînement\n",
        "      optimizer : l'optimiseur qui est utilisé pour mettre à jour les poids du modèle\n",
        "      epochs : le nombre d'époques pour lesquelles entraîner le modèle\n",
        "\n",
        "    Sortie :\n",
        "      end_train, chaîne de caractères indiquant l'état de l'entraînement\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        #On vérifie si le GPU est dispo (plus rapide), sinon on utilise le CPU\n",
        "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        #On déplace le modèle sur le GPU ou le CPU\n",
        "        model.to(device)\n",
        "\n",
        "        #On boucle sur le nombre d'époques donné en entrée :\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            #On met le modèle en mode \"entraînement\"\n",
        "            model.train()\n",
        "\n",
        "            #Ici on initialise la perte totale pour l'époque en cours\n",
        "            perte_totale=0\n",
        "\n",
        "            #On boucle sur les lots dans le dataloader d'entraînement\n",
        "            for batch in tqdm(train_dataloader):\n",
        "                b_input_ids=batch[0].to(device)\n",
        "                b_input_mask=batch[1].to(device)\n",
        "                b_labels=batch[2].to(device)\n",
        "\n",
        "                #Réinitialisation des gradients du modèle\n",
        "                model.zero_grad()\n",
        "\n",
        "                #On passe les données au modèle pour obtenir des sorties\n",
        "                outputs=model(b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n",
        "                loss=outputs.loss #On obtient ici la perte à partir des sorties obtenues\n",
        "\n",
        "                #Ajout de cette perte à la perte totale\n",
        "                perte_totale+=loss.item()\n",
        "\n",
        "                #On calcule les gradients\n",
        "                loss.backward()\n",
        "\n",
        "                #On met à jour le poids du modèle avec l'optimiseur\n",
        "                optimizer.step()\n",
        "\n",
        "            #Retour sur la perte totale pour cette époque\n",
        "            print(f'Epoque : {epoch+1}/{epochs}, Perte : {perte_totale}')\n",
        "\n",
        "        end_train=\"Entraînement terminé !\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "      end_train=f\"Entraînement impossible : {str(e)}\"\n",
        "\n",
        "    return end_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofwlj9M7ZoK9"
      },
      "outputs": [],
      "source": [
        "#On crée les dataloaders\n",
        "train_data=TensorDataset(train_inputs,train_masks,train_labels)\n",
        "train_dataloader=DataLoader(train_data,batch_size=32,shuffle=True)\n",
        "\n",
        "#Optimiseur\n",
        "optimizer=torch.optim.AdamW(modele.parameters(),lr=2e-5)\n",
        "\n",
        "#Entraînement du modèle\n",
        "train_model(modele,train_dataloader,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY-QVpJt0prX"
      },
      "source": [
        "##Test du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynXWvxO_ZpZ9"
      },
      "outputs": [],
      "source": [
        "def test_model(model,dataloader):\n",
        "    \"\"\"\n",
        "    Cette fonction teste notre modèle sur des données test.\n",
        "\n",
        "    Entrées :\n",
        "      model : le modèle à tester\n",
        "      dataloader : le dataloader contenant les données de test\n",
        "\n",
        "    Sortie :\n",
        "      les étiquettes prédites par notre modèle\n",
        "    \"\"\"\n",
        "    try:\n",
        "        #On vérifie si le GPU est dispo (plus rapide), sinon on utilise le CPU\n",
        "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        #On bouge le modèle sur le GPU ou CPU\n",
        "        model.to(device)\n",
        "\n",
        "        #On met le modèle en mode \"eval\" (pas de maj des gradients)\n",
        "        model.eval()\n",
        "\n",
        "        #Pour stocker les prédictions faites par notre modèle\n",
        "        predictions=[]\n",
        "\n",
        "        #On boucle sur les lots des données dans le dataloader test\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                inputs=batch[0].to(device)\n",
        "                masks=batch[1].to(device)\n",
        "\n",
        "                #On passe les données au modèle pour obtenir des logits\n",
        "                outputs=model(inputs,attention_mask=masks)\n",
        "                logits=outputs.logits\n",
        "                _,predicted_labels=torch.max(logits, 1) #fct argmax pour obtenir étiquettes prédites\n",
        "\n",
        "                #On ajoute les labels prédits à notre liste\n",
        "                predictions.extend(predicted_labels.tolist())\n",
        "\n",
        "        #print(\"Indices avant conversion : \",predictions)\n",
        "\n",
        "        try:\n",
        "          #Ici on convertit les indices prédits (0, 1, 2) en étiquettes (positif, négatif et neutre)\n",
        "          label_dict={\"positif\":0,\"negatif\":1,\"neutre\":2}\n",
        "          label_map_inverse={v: k for k, v in label_dict.items()} #on inverse le dico\n",
        "          predicted_labels=[label_map_inverse[prediction] for prediction in predictions]\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"La conversion des indices en labels est impossible : \",str(e))\n",
        "\n",
        "\n",
        "        end_test=\"Test terminé !\"\n",
        "        #print(\"Labels prédits : \",predicted_labels)\n",
        "\n",
        "        count_positif=predicted_labels.count(\"positif\")\n",
        "        count_negatif=predicted_labels.count(\"negatif\")\n",
        "        count_neutre=predicted_labels.count(\"neutre\")\n",
        "\n",
        "    except Exception as e:\n",
        "        end_test=f\"Test impossible : {str(e)}\"\n",
        "        predicted_labels=0\n",
        "        count_positif=0\n",
        "        count_negatif=0\n",
        "        count_neutre=0\n",
        "\n",
        "    print(end_test)\n",
        "    print(\"Avis positifs prédits :\",count_positif)\n",
        "    print(\"Avis négatifs prédits :\",count_negatif)\n",
        "    print(\"Avis neutres prédits :\",count_neutre)\n",
        "\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATWJniufZuuF"
      },
      "outputs": [],
      "source": [
        "#On crée un dataloader pour les données de test\n",
        "test_data=TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_dataloader=DataLoader(test_data, batch_size=32)\n",
        "\n",
        "#Utilisation de ce dataloader pour tester le modèle\n",
        "test_predictions=test_model(modele,test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNHZ6IQ30tB_"
      },
      "source": [
        "##Evaluation du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmqHVkDDZr89"
      },
      "outputs": [],
      "source": [
        "def evaluate(true_labels:list,predicted_labels:list):\n",
        "    \"\"\"\n",
        "    Cette fonction évalue les performances du modèle sur ses prédictions par rapport aux vraies étiquettes.\n",
        "\n",
        "    Entrées :\n",
        "      true_labels : vraies étiquettes des données\n",
        "      predicted_labels : étiquettes prédites par le modèle\n",
        "\n",
        "    Sortie :\n",
        "      les métriques : précision, rappel et Fmesure\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Calcul de la précision, du rappel et de la Fmesure\n",
        "      precision=precision_score(true_labels,predicted_labels, average='weighted',zero_division=0)\n",
        "      rappel=recall_score(true_labels,predicted_labels,average='weighted',zero_division=0)\n",
        "      fmesure=f1_score(true_labels,predicted_labels,average='weighted',zero_division=0)\n",
        "\n",
        "      #print(true_labels)\n",
        "      #print(predicted_labels)\n",
        "\n",
        "      count_positif_pred=predicted_labels.count(\"positif\")\n",
        "      count_negatif_pred=predicted_labels.count(\"negatif\")\n",
        "      count_neutre_pred=predicted_labels.count(\"neutre\")\n",
        "      count_positif_true=true_labels.count(\"positif\")\n",
        "      count_negatif_true=true_labels.count(\"negatif\")\n",
        "      count_neutre_true=true_labels.count(\"neutre\")\n",
        "\n",
        "      print(\"PREDICTIONS :\")\n",
        "      print(\"\\tAvis positifs prédits :\",count_positif_pred)\n",
        "      print(\"\\tAvis négatifs prédits :\",count_negatif_pred)\n",
        "      print(\"\\tAvis neutres prédits :\",count_neutre_pred)\n",
        "      print(\"\\tTOTAL : \",len(predicted_labels),\"\\n\")\n",
        "      print(\"=\"*50)\n",
        "      print(\"LABELS REELS :\")\n",
        "      print(\"\\tAvis positifs réels :\",count_positif_true)\n",
        "      print(\"\\tAvis négatifs réels :\",count_negatif_true)\n",
        "      print(\"\\tAvis neutres réels :\",count_neutre_true)\n",
        "      print(\"\\tTOTAL : \",len(true_labels),\"\\n\")\n",
        "      print(\"=\"*50)\n",
        "\n",
        "      metriques=print(f\"EVALUATION DU MODELE :\\n\\tPrécision : {precision}\\n\\tRappel : {rappel}\\n\\tFmesure : {fmesure}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "      metriques=f\"Evaluation impossible : {str(e)}\"\n",
        "\n",
        "    return metriques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_Oorx-iZvU9"
      },
      "outputs": [],
      "source": [
        "true_labels=[review['label'] for review in labeled_test_reviews]\n",
        "evaluate(true_labels,test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Oj2xJ3ZwsO"
      },
      "outputs": [],
      "source": [
        "print(true_labels)\n",
        "print(test_predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}